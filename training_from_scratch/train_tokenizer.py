"""
Training From Scratch â€” Step 0: Train a BPE Tokenizer
Builds a custom tokenizer from ALL datasets so every model
shares the same vocabulary learned entirely from our data.
"""

import json
import sys
import os
from pathlib import Path

sys.path.insert(0, os.path.dirname(__file__))

from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors, decoders
from tokenizers.normalizers import NFD, StripAccents, Sequence as NormSequence

from config_scratch import TOKENIZER_CONFIG, VOCAB_DIR, SPLITS_DIR, RAW_DIR, ALL_DATASETS


def collect_text_files():
    """Gather all text data from splits and raw directories."""
    text_files = []
    if SPLITS_DIR.exists():
        for jsonl in SPLITS_DIR.rglob("*.jsonl"):
            text_files.append(str(jsonl))
    if RAW_DIR.exists():
        for ext in ["*.txt", "*.jsonl", "*.json", "*.csv", "*.tsv"]:
            for f in RAW_DIR.rglob(ext):
                text_files.append(str(f))
    return text_files


def text_iterator(file_paths, max_texts=5_000_000):
    """Yield text strings from data files for tokenizer training."""
    count = 0
    for fpath in file_paths:
        try:
            with open(fpath, "r", encoding="utf-8", errors="ignore") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        for key in ["text", "input", "output", "sentence", "sentence1",
                                    "sentence2", "text_a", "text_b", "question1",
                                    "question2", "content", "document"]:
                            if key in obj and isinstance(obj[key], str):
                                yield obj[key]
                                count += 1
                                if count >= max_texts:
                                    return
                    except (json.JSONDecodeError, TypeError):
                        if len(line) > 10:
                            yield line
                            count += 1
                            if count >= max_texts:
                                return
        except Exception as e:
            print(f"  [WARN] Could not read {fpath}: {e}")


def train_tokenizer():
    """Train a BPE tokenizer from scratch on all project data."""
    print("=" * 60)
    print("  STEP 0: Training BPE Tokenizer From Scratch")
    print("=" * 60)

    tokenizer = Tokenizer(models.BPE(unk_token="[UNK]"))
    tokenizer.normalizer = NormSequence([NFD(), StripAccents()])
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)
    tokenizer.decoder = decoders.ByteLevel()
    tokenizer.post_processor = processors.TemplateProcessing(
        single="[CLS] $A [SEP]",
        pair="[CLS] $A [SEP] $B:1 [SEP]:1",
        special_tokens=[("[CLS]", 2), ("[SEP]", 3)],
    )

    trainer = trainers.BpeTrainer(
        vocab_size=TOKENIZER_CONFIG["vocab_size"],
        min_frequency=TOKENIZER_CONFIG["min_frequency"],
        special_tokens=TOKENIZER_CONFIG["special_tokens"],
        show_progress=True,
    )

    print("\n  Collecting text data from all datasets...")
    text_files = collect_text_files()
    print(f"  Found {len(text_files)} data files")

    if not text_files:
        print("  [ERROR] No data files found. Run setup_all.py first.")
        print("  Training on minimal fallback corpus...")
        tokenizer.train_from_iterator(
            [
                "The quick brown fox jumps over the lazy dog.",
                "Artificial intelligence has transformed technology.",
                "This text was generated by a machine learning model.",
                "Plagiarism detection requires comparing document similarity.",
                "Natural language processing enables text understanding.",
            ] * 1000,
            trainer=trainer,
        )
    else:
        print(f"  Training tokenizer on up to 5M text samples...")
        tokenizer.train_from_iterator(text_iterator(text_files), trainer=trainer)

    pad_id = tokenizer.token_to_id("[PAD]")
    tokenizer.enable_padding(pad_id=pad_id, pad_token="[PAD]")
    tokenizer.enable_truncation(max_length=TOKENIZER_CONFIG["max_length"])

    save_path = VOCAB_DIR / "tokenizer.json"
    tokenizer.save(str(save_path))

    config_path = VOCAB_DIR / "tokenizer_config.json"
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(TOKENIZER_CONFIG, f, indent=2)

    print(f"\n  Tokenizer trained!")
    print(f"  Vocab size: {tokenizer.get_vocab_size()}")
    print(f"  Saved to: {save_path}")
    return tokenizer


if __name__ == "__main__":
    train_tokenizer()
