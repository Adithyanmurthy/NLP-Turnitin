Phase,Person,Task,Model/Component,Parameters,Train Samples,Batch Size,Effective Batch,Steps per Epoch,Epochs,Sec per Step,Estimated Time (hours),Notes
Setup,—,Clone Repository,git clone,—,—,—,—,—,—,—,0.05,Small repo ~3MB
Setup,—,Install Packages,pip install (27 packages),—,—,—,—,—,—,—,0.25,Includes torch==2.0.1+cu117
Setup,—,Download Datasets,11 auto-download datasets,—,~16 GB total,—,—,—,—,—,1.00,RAID 14.6GB is the largest
Setup,—,Preprocess Datasets,11 datasets into train/val/test splits,—,~1.5M records total,—,—,—,—,—,0.50,RAID capped at 500K; WikiSplit/ParaNMT at 300K
Setup,—,Download Person 3 Data,train/val/test JSONL,—,~15 MB,—,—,—,—,—,0.02,Already generated by dataset_downloader.py
Setup,—,Download Pre-trained Models,10 models from HuggingFace,—,~80 GB total,—,—,—,—,—,2.00,Depends on network speed; DIPPER alone is ~44GB
Setup,—,SETUP TOTAL,—,—,—,—,—,—,—,—,3.82,—
Training,Person 1,Train DeBERTa-v3-large,microsoft/deberta-v3-large,435M,~60K (3 datasets × 20K),8,16 (×2 grad accum),3750,3,0.50,1.56,Primary AI classifier
Training,Person 1,Train RoBERTa-large,roberta-large,355M,~60K (3 datasets × 20K),8,16 (×2 grad accum),3750,3,0.40,1.25,Ensemble member
Training,Person 1,Train Longformer-base,allenai/longformer-base-4096,149M,~5K (RAID filtered >2000 chars),2,16 (×8 grad accum),312,3,1.50,0.39,Long document specialist; fewer samples after filtering
Training,Person 1,Train XLM-RoBERTa-large,xlm-roberta-large,560M,~20K (1 dataset: faidset),8,16 (×2 grad accum),1250,3,0.50,0.52,Multilingual detection
Training,Person 1,Train Meta-Classifier,Logistic Regression (sklearn),tiny,~20K (val set predictions),—,—,—,—,—,0.30,Combines 4 model predictions; includes inference on val set
Training,Person 1,Evaluate Ensemble,All 4 models + meta-classifier,—,~20K test samples,—,—,—,—,—,0.30,Generates evaluation_report.json
Training,Person 1,P1 TOTAL,—,—,—,—,—,—,—,—,4.32,—
Training,Person 2,Train Sentence-BERT,sentence-transformers/all-mpnet-base-v2,110M,~115K (STS 5.7K + PAWS 49K + QQP 100K),16,16,7187,3,0.10,0.60,CosineSimilarityLoss; fast training
Training,Person 2,Train Cross-Encoder,cross-encoder/nli-deberta-v3-large,435M,~15K (PAWS 8K + MRPC 3.7K + STS 3K),16,16,937,3,0.30,0.23,Binary classification; small dataset
Training,Person 2,P2 TOTAL,—,—,—,—,—,—,—,—,0.83,—
Training,Person 3,Train Flan-T5-XL,google/flan-t5-xl,3B,~50K (Person 3 train.jsonl),1,4 (×4 grad accum),12500,3,2.00,6.94,Seq2seq humanization; batch_size=1 for 20GB VRAM
Training,Person 3,Train PEGASUS-large,google/pegasus-large,568M,~50K (Person 3 train.jsonl),4,16 (×4 grad accum),3125,3,0.80,2.08,Seq2seq paraphrasing
Training,Person 3,Train Mistral-7B QLoRA,mistralai/Mistral-7B-v0.3,7B (4-bit quantized),~50K (Person 3 train.jsonl),2,8 (×4 grad accum),6250,2,2.00,6.94,QLoRA with paged_adamw_8bit; 4-bit NF4 quantization
Training,Person 3,Setup DIPPER,kalpeshk2011/dipper-paraphraser-xxl,11B,—,—,—,—,—,—,0.00,SKIPPED — 11B params exceeds 20GB VRAM
Training,Person 3,P3 TOTAL,—,—,—,—,—,—,—,—,15.97,—
GRAND TOTAL,—,—,—,—,—,—,—,—,—,—,24.94,—
,,,,,,,,,,,,,
HARDWARE SPECS,,,,,,,,,,,,,
GPU,NVIDIA A100-SXM4-40GB MIG 3g.20gb (~20GB usable VRAM),,,,,,,,,,,,
CPU,Server-grade (Kubeflow pod),,,,,,,,,,,,
RAM,~32GB,,,,,,,,,,,,
Disk,417GB free on overlay filesystem,,,,,,,,,,,,
CUDA,Driver 11.x with torch 2.0.1+cu117,,,,,,,,,,,,
Python,3.8,,,,,,,,,,,,
,,,,,,,,,,,,,
BREAKDOWN SUMMARY,,,,,,,,,,,,,
Phase,Estimated Hours,,,,,,,,,,,,
Setup (clone + install + download + preprocess),3.82,,,,,,,,,,,,
Person 1 Training (AI Detection — 4 models + meta + eval),4.32,,,,,,,,,,,,
Person 2 Training (Plagiarism — 2 models),0.83,,,,,,,,,,,,
Person 3 Training (Humanization — 3 models),15.97,,,,,,,,,,,,
GRAND TOTAL,24.94,,,,,,,,,,,,
